{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This code is designed to evaluate the performance of machine learning classifiers: Random Forest and Logistic Regression.\n",
    "\n",
    "### Experiment Logic:\n",
    "- **Data Preprocessing**\n",
    "- **Model Tuning**\n",
    "- **Evaluation and Comparison**\n",
    "- **Overfitting Analysis**\n",
    "- **Final Model Selection**\n",
    "\n",
    "The code is divided into two main modules:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Method Definition Module Overview\n",
    "\n",
    "This section includes the following segments:\n",
    "\n",
    "- **Method of Monitoring System Resource Usage**\n",
    "- **Method of Data Loading**\n",
    "- **Method of Filtering Invalid Parameter Combinations**\n",
    "- **Model and Hyperparameter Definitions**\n",
    "- **Method of Hyperparameter Search**\n",
    "- **Method of Training and Evaluation**\n",
    "- **Method of Plotting ROC and PR Curves**\n",
    "- **Method of Test Set Evaluation**\n",
    "- **Method of Plotting Learning Curves**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Execution Module Overview\n",
    "\n",
    "This section includes the following segments:\n",
    "\n",
    "- **Data Loading and Splitting**\n",
    "- **Hyperparameter Search**\n",
    "- **K-Fold Cross Validation**\n",
    "- **Statistical Test Results**\n",
    "- **Model Selection and Final Testing**\n",
    "- **Overfitting Detection**\n",
    "- **Random Forest Manual Hyperparameter Tuning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages\n",
    "Numpy and pandas provide numerical computation and structured data processing capabilities; Matplotlib and Seaborn are used for data visualization, including model evaluation metrics such as ROC and PR curves. Time, Psutil, and GPUtil enable system monitoring, including execution time measurement, CPU/memory tracking, and GPU utilization analysis. The SciPy module supports statistical analysis with tools like the t-test. The sklearn module implements the complete machine learning pipeline—ensuring balanced data distribution through StratifiedKFold, performing hyperparameter tuning with RandomizedSearchCV, integrating data standardization (StandardScaler) and model training via Pipeline, and computing key performance metrics such as accuracy, precision, recall, F1-score, AUC, and average precision. RandomForestClassifier and LogisticRegression serve as the primary classification models, providing a combination of ensemble-based learning and interpretable linear classification. Additionally, warnings are suppressed to avoid unnecessary alerts, and a fixed random seed is set for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GPUtil'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsutil\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mGPUtil\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ttest_ind\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'GPUtil'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, roc_auc_score, average_precision_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Method Definition Module Overview\r\n",
    "\r\n",
    "This section includes the following segments:\r\n",
    "\r\n",
    "- **Method of Monitoring System Resource Usage**\r\n",
    "- **Method of Data Loading**\r\n",
    "- **Method of Filtering Invalid Parameter Combinations**\r\n",
    "- **Model and Hyperparameter Definitions**\r\n",
    "- **Method of Hyperparameter Search**\r\n",
    "- **Method of Training and Evaluation**\r\n",
    "- **Method of Plotting ROC and PR Curves**\r\n",
    "- **Method of Test Set Evaluation**\r\n",
    "- **Method of Plotting Learning Curves**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Method of Monitoring System Resource Usage\n",
    "The get_system_usage function monitors CPU, memory, and GPU utilization.\n",
    "This function provides a snapshot of system resource consumption, aiding in performance monitoring during machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method of Monitoring system resource usage\n",
    "def get_system_usage():\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)\n",
    "    mem_usage = psutil.virtual_memory().percent\n",
    "    gpu_usage = 0.0  # set to 0.0 by default\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        if gpus:\n",
    "            gpu_usage = gpus[0].load * 100  # getting the percent of first GPU\n",
    "    except:\n",
    "        gpu_usage = \"N/A\"  \n",
    "    return cpu_usage, mem_usage, gpu_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Method of Data Loading\n",
    "The load_data function reads a CSV file, extracts feature variables (X) and target labels (y) for a binary classification task, \n",
    "and ensures numerical data is in float32 format for efficient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, header=0, sep=';')#we have headerin first row,skip it; separate data with ;\n",
    "    X = data.iloc[:, 1:-1].astype(np.float32)  # first column is id, last one is target value\n",
    "    y = data.iloc[:, -1]  # binary classification\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Method of Filtering Invalid Parameter Combinations\n",
    "The filter_params function filters out incompatible solver-penalty combinations for Logistic Regression, ensuring that only valid combinations \n",
    "(e.g., lbfgs with l2) are included in the parameter grid. For other models, such as Random Forest, it directly returns the original parameter grid \n",
    "without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method of filtering invalid parameter combinations\n",
    "def filter_params(model_name, param_grid):\n",
    "    # C: Regularization strength in Logistic Regression, controlling overfitting.\n",
    "#solver: The algorithm used to optimize the Logistic Regression model (e.g., lbfgs, liblinear).\n",
    "#penalty: The type of regularization applied (e.g., l1, l2).\n",
    "    if model_name == \"Logistic Regression\":\n",
    "        filtered_grid = {\"C\": [], \"solver\": [], \"penalty\": []}\n",
    "        for C in param_grid[\"C\"]:\n",
    "            for solver in param_grid[\"solver\"]:\n",
    "                for penalty in param_grid[\"penalty\"]:\n",
    "                    if (solver == \"lbfgs\" and penalty == \"l1\") or \\\n",
    "                       (solver == \"liblinear\" and penalty == \"elasticnet\"):\n",
    "                        continue  # skip invalid one\n",
    "                    filtered_grid[\"C\"].append(C)\n",
    "                    filtered_grid[\"solver\"].append(solver)\n",
    "                    filtered_grid[\"penalty\"].append(penalty)\n",
    "        return filtered_grid\n",
    "    return param_grid  # return directly if it is Random Forest \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model and Hyperparameter Definitions\n",
    "The models dictionary defines two machine learning models—Logistic Regression and Random Forest—with their respective hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the search area of hyperparameters of models\n",
    "models = {\n",
    "    'Logistic Regression': {\n",
    "        'class': LogisticRegression,\n",
    "        'param_grid': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['liblinear', 'lbfgs', 'saga'],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'class': RandomForestClassifier,\n",
    "        'param_grid': {\n",
    "            'n_estimators': [500, 1000],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'bootstrap': [True],\n",
    "            'random_state': [RANDOM_SEED],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Method of Hyperparameter Search\n",
    "The hyperparameter_search function performs hyperparameter tuning using RandomizedSearchCV. For Logistic Regression, it creates a pipeline that includes data scaling (StandardScaler) and the model, and filters the parameter grid to ensure compatibility. For Random Forest, it directly uses the provided parameter grid. The function searches for the best hyperparameters by evaluating the model using 3-fold cross-validation and the roc_auc scoring metric, then returns the best model and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method of hyperparameter search\n",
    "def hyperparameter_search(model_name, model_class, param_grid, X_train, y_train):\n",
    "    #search using train_valid set first to find the best parameters\n",
    "    print(f\"\\n🔍 Searching best hyperparameters for {model_name}...\")\n",
    "#in logistic regression, data need to be standardization first \n",
    "    if model_name == 'Logistic Regression':\n",
    "        pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', model_class(random_state=RANDOM_SEED))\n",
    "        ])\n",
    "        filtered_grid = filter_params(model_name, param_grid)\n",
    "        param_grid_fixed = {f\"model__{k}\": v for k, v in filtered_grid.items()} \n",
    "    else:\n",
    "        pipeline = model_class(random_state=RANDOM_SEED)\n",
    "        param_grid_fixed = param_grid  # Random Forest using the dictionary directly\n",
    "# speed up using random search\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline, param_distributions=param_grid_fixed, n_iter=10,\n",
    "        cv=3, scoring='roc_auc', n_jobs=4, random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    search.fit(X_train, y_train)\n",
    "    print(f\"Best hyperparameters for {model_name}: {search.best_params_}\")\n",
    "    \n",
    "    return search.best_estimator_, search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Method of Training and Evaluation\n",
    "The train_and_evaluate function performs K-fold cross-validation to train and evaluate the model. For each fold, it trains the model on the training\n",
    "subset and evaluates it on the validation subset, recording performance metrics like Recall, F1-score, AUC, AP, and Precision. It also tracks CPU,\n",
    "memory, and GPU usage, along with the time taken for training each fold. After completing all folds, it computes and prints average metrics,\n",
    "total training time, and average resource usage. The function then plots the ROC and precision-recall curves and returns the evaluation results across\n",
    "all folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def train_and_evaluate(model_name, best_model, X, y, cv):\n",
    "    #using k-folds to train and validate, records the results of training that are needed. \n",
    "    all_fpr, all_tpr, all_precision, all_recall = [], [], [], []\n",
    "    train_errors, val_errors = [], []\n",
    "    auc_scores, ap_scores, f1_scores, precision_scores, recall_scores = [], [], [], [], []\n",
    "    cpu_usages, mem_usages, gpu_usages, training_times = [], [], [], []\n",
    "    \n",
    "    print(f\"\\n=== Running {model_name} ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        print(f\"\\n=== Fold {fold+1} ===\")\n",
    "\n",
    "        fold_start_time = time.time()\n",
    "        cpu_start, mem_start, gpu_start = get_system_usage()\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        model = best_model  # using the best model got from hyperparameter search\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)    \n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        train_f1 = f1_score(y_train, y_train_pred)\n",
    "        val_f1 = f1_score(y_val, y_val_pred)\n",
    "       # used for plot learning curves\n",
    "        train_errors.append(1 - train_f1)  \n",
    "        val_errors.append(1 - val_f1)   \n",
    "\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_val, y_proba)\n",
    "        prec, rec, _ = precision_recall_curve(y_val, y_proba)\n",
    "\n",
    "        auc_score = roc_auc_score(y_val, y_proba)\n",
    "        ap_score = average_precision_score(y_val, y_proba)\n",
    "        precision = precision_score(y_val, y_val_pred)\n",
    "        recall = recall_score(y_val, y_val_pred)\n",
    "\n",
    "        #getting a list of data for each by storing the data\n",
    "        auc_scores.append(auc_score)         \n",
    "        ap_scores.append(ap_score)\n",
    "        f1_scores.append(val_f1)\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "\n",
    "        all_fpr.append(fpr)\n",
    "        all_tpr.append(tpr)\n",
    "        all_precision.append(prec)\n",
    "        all_recall.append(rec)\n",
    "        \n",
    "        cpu_end, mem_end, gpu_end = get_system_usage()\n",
    "        #calculating time for each fold\n",
    "        fold_training_time = time.time() - fold_start_time\n",
    "        cpu_usage = (cpu_start + cpu_end) / 2\n",
    "        mem_usage = (mem_start + mem_end) / 2\n",
    "        gpu_usage = (gpu_start + gpu_end) / 2 if gpu_start != \"N/A\" else \"N/A\"\n",
    "\n",
    "        cpu_usages.append(cpu_usage)\n",
    "        mem_usages.append(mem_usage)\n",
    "        gpu_usages.append(gpu_usage)\n",
    "        training_times.append(fold_training_time)\n",
    "        \n",
    "        print(f\"Fold {fold+1}: Recall = {recall:.3f},F1 = {val_f1:.3f}, \"\n",
    "              f\"AUC = {auc_score:.3f}, AP = {ap_score:.3f}, Precision = {precision:.3f}, \"\n",
    "              f\"Training Time = {fold_training_time:.2f}s, CPU = {cpu_usage:.2f}%, \"\n",
    "              f\"Memory = {mem_usage:.2f}%, GPU = {gpu_usage:.2f}%\")\n",
    "        \n",
    "    #calculating the total time    \n",
    "    end_time = time.time()\n",
    "    total_training_time = end_time - start_time\n",
    "    avg_cpu_usage = np.mean(cpu_usages)\n",
    "    avg_mem_usage = np.mean(mem_usages)\n",
    "    avg_gpu_usage = np.mean(gpu_usages) if gpu_usages else \"N/A\"\n",
    "    \n",
    "    print(f\"\\n {model_name} Total Training Time: {total_training_time:.2f}s | \"\n",
    "          f\"Avg CPU: {avg_cpu_usage:.2f}% | Avg Memory: {avg_mem_usage:.2f}% | Avg GPU: {avg_gpu_usage:.2f}%\")    \n",
    "\n",
    "    \n",
    "    print(f\"\\n Final {model_name} Avg Recall: {np.mean(recall_scores):.3f}, Avg F1: {np.mean(f1_scores)}, Avg AUC: {np.mean(auc_scores):.3f}, Avg AP: {np.mean(ap_scores):.3f}, Avg Precision:{np.mean(precision_scores)}\")\n",
    "\n",
    "    plot_roc_pr_curves(all_fpr, all_tpr, all_precision, all_recall, model_name)\n",
    "\n",
    "# return lists of data, which will be used in statistic test\n",
    "    return auc_scores, ap_scores, train_errors, val_errors, f1_scores, recall_scores, training_times, cpu_usages, mem_usages, gpu_usages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Method of Plotting ROC and PR Curves\n",
    "The plot_roc_pr_curves function plots the ROC (Receiver Operating Characteristic) and PR (Precision-Recall) curves for all folds of cross-validation.\n",
    "The ROC curve is plotted on the left, showing the False Positive Rate vs True Positive Rate, with a reference line for random guessing.\n",
    "The PR curve is plotted on the right, showing Recall vs Precision. Each fold’s curve is labeled, and the plot is displayed with appropriate titles\n",
    "and legends for the model name. The function helps in visually assessing the model's performance across different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method of plotting roc and pr vurves\n",
    "def plot_roc_pr_curves(all_fpr, all_tpr, all_precision, all_recall, model_name):\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # plot ROC curve\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(len(all_fpr)):\n",
    "        plt.plot(all_fpr[i], all_tpr[i], lw=1.5, label=f'Fold {i+1}')\n",
    "    \n",
    "    # Add the y = x reference line (random guessing baseline)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1, label='Random Guess')\n",
    "    \n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    # plot PR curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(len(all_precision)):\n",
    "        plt.plot(all_recall[i], all_precision[i], lw=1.5, label=f'Fold {i+1}')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{model_name} - Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Method of Test Set Evaluation\n",
    "This method evaluates the final model on the test set by calculating metrics such as AUC, average precision (AP),\n",
    "F1 score, precision, and recall. It also tracks evaluation time and system resource usage (CPU, memory, GPU). \n",
    "Additionally, it plots the ROC curve and Precision-Recall curve, and prints the final evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method of test set evaluation\n",
    "def evaluate_on_test_set(model, X_test, y_test, model_name):\n",
    "    #start the timing\n",
    "    start_time = time.time()\n",
    "    cpu_start, mem_start, gpu_start = get_system_usage()\n",
    "\n",
    "    \n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "    auc_score = roc_auc_score(y_test, y_test_proba)\n",
    "    ap_score = average_precision_score(y_test, y_test_proba)\n",
    "    f1 = f1_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred)\n",
    "    recall = recall_score(y_test, y_test_pred)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_test_proba)\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    cpu_end, mem_end, gpu_end = get_system_usage()\n",
    "\n",
    "    test_time = end_time - start_time\n",
    "    test_cpu = (cpu_start + cpu_end) / 2\n",
    "    test_mem = (mem_start + mem_end) / 2\n",
    "    test_gpu = (gpu_start + gpu_end) / 2 if gpu_start != \"N/A\" else \"N/A\"\n",
    "\n",
    "    print(f\"\\n Final Model Evaluation Time: {test_time:.2f}s | CPU: {test_cpu:.2f}% | Memory: {test_mem:.2f}% | GPU: {test_gpu}%\")\n",
    "\n",
    "\n",
    "    print(f\"\\n {model_name} Final Test Results:\")\n",
    "    print(f\" Recall: {recall:.3f},F1: {f1:.3f}, AUC: {auc_score:.3f}, AP: {ap_score:.3f},  Precision: {precision:.3f}\")\n",
    "\n",
    "    plot_roc_pr_curves([fpr], [tpr], [prec], [rec], f\"{model_name} - Test Set\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Method of Plotting Learning Curves\n",
    "This method plots three learning curves to assess model performance: one representing the training F1 scores using 5-fold cross-validation, another for the validation F1 scores from cross-validation, and a third showing the test F1 scores to evaluate the model's generalization ability as the training size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(final_model, X_train, y_train, X_test, y_test, model_name):\n",
    "\n",
    "\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        final_model, X_train, y_train, cv=5, scoring='f1',\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=4\n",
    "    )\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "\n",
    "    test_scores = []  #store the test results\n",
    "    for train_size in train_sizes:\n",
    "        X_subtrain = X_train[:int(train_size)]\n",
    "        y_subtrain = y_train[:int(train_size)]\n",
    "\n",
    "        model = final_model.fit(X_subtrain, y_subtrain)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        test_scores.append(f1_score(y_test, y_test_pred))\n",
    "\n",
    "    # plot curves\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    #training curves and validation curves\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training Score (CV)\")\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', color=\"b\", label=\"Validation Score (CV)\")\n",
    "\n",
    "    # testing curves\n",
    "    plt.plot(train_sizes, test_scores, 'o-', color=\"g\", label=\"Test Score\")\n",
    "\n",
    "    plt.xlabel(\"Training Samples\")\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.title(f\"{model_name} Learning Curves\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Execution Module Overview\r\n",
    "\r\n",
    "This section includes the following segments:\r\n",
    "\r\n",
    "- **Data Loading and Splitting**\r\n",
    "- **Hyperparameter Search**\r\n",
    "- **K-Fold Cross Validation**\r\n",
    "- **Statistical Test Results**\r\n",
    "- **Model Selection and Final Testing**\r\n",
    "- **Overfitting Detection**\r\n",
    "- **Random Forest Manual Hyperparameter Model Selection**\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Loading and Splitting\n",
    "This section involves loading the dataset from the specified file path, followed by splitting the data into features (X) and target labels (y). The dataset is then further split into training-validation and test sets, with 10% of the data reserved for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"cardio_train.csv\"\n",
    "X, y = load_data(file_path)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=RANDOM_SEED, stratify=y)\n",
    "k_folds = min(10, len(X_train_val) // 5000)\n",
    "cv = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Hyperparameter Search\n",
    "This section performs hyperparameter search for each model in the models dictionary. It iterates over the models, applying the hyperparameter_search function to find the best model and corresponding hyperparameters. The results are stored in dictionaries, with best_models holding the optimal models and best_params storing the best parameter sets for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_models = {}  \n",
    "best_params = {}  \n",
    "for model_name, model_info in models.items():\n",
    "    best_models[model_name], best_params[model_name] = hyperparameter_search(\n",
    "        model_name, model_info['class'], model_info['param_grid'], X_train_val, y_train_val\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 K-Fold Cross-Validation\n",
    "This section performs K-fold cross-validation for each model using the best model found from the hyperparameter search. It iterates over the models and evaluates their performance using the train_and_evaluate function. The results, including performance metrics, are stored in the results dictionary for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {}\n",
    "for name, info in models.items():\n",
    "    results[name] = train_and_evaluate(name, best_models[name], X_train_val, y_train_val, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Statistical Testing\n",
    "This section performs statistical tests to compare the performance metrics of Logistic Regression and Random Forest models. Using a t-test, it evaluates the significance of differences across various metrics, including Recall, F1-score, AUC, AP, training time, and system resource usage (CPU, memory, and GPU). The results are printed, highlighting whether a significant difference was detected for each metric based on the p-value threshold of 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "metrics = [\"Recall\", \"F1-score\", \"AUC\", \"AP\", \"Training Time (s)\", \"CPU Usage (%)\", \"Memory Usage (%)\", \"GPU Usage (%)\"]\n",
    "logistic_training_time, logistic_cpu_usage, logistic_memory_usage, logistic_gpu_usage = results['Logistic Regression'][6:]\n",
    "rf_training_time, rf_cpu_usage, rf_memory_usage, rf_gpu_usage = results['Random Forest'][6:]\n",
    "\n",
    "metrics = [\"Recall\", \"F1-score\", \"AUC\", \"AP\", \"Training Time (s)\", \"CPU Usage (%)\", \"Memory Usage (%)\", \"GPU Usage (%)\"]\n",
    "\n",
    "logistic_results = [\n",
    "    results['Logistic Regression'][4],  # Recall\n",
    "    results['Logistic Regression'][3],  # F1-score\n",
    "    results['Logistic Regression'][0],  # AUC\n",
    "    results['Logistic Regression'][1],  # AP\n",
    "    logistic_training_time,             # training time\n",
    "    logistic_cpu_usage,                  # CPU \n",
    "    logistic_memory_usage,               # memory\n",
    "    logistic_gpu_usage                    # GPU \n",
    "]\n",
    "\n",
    "random_forest_results = [\n",
    "    results['Random Forest'][4],  # Recall\n",
    "    results['Random Forest'][3],  # F1-score\n",
    "    results['Random Forest'][0],  # AUC\n",
    "    results['Random Forest'][1],  # AP\n",
    "    rf_training_time,             # training time\n",
    "    rf_cpu_usage,                  # CPU \n",
    "    rf_memory_usage,               # memory\n",
    "    rf_gpu_usage                    # GPU \n",
    "]\n",
    "\n",
    "print(\"\\n🎯 Statistical Test Results (Logistic Regression vs Random Forest)\")\n",
    "for i in range(len(metrics)):\n",
    "    t_stat, p_value = ttest_ind(logistic_results[i], random_forest_results[i])\n",
    "\n",
    "    print(f\"\\n📌 {metrics[i]}:\")\n",
    "    print(f\"   T-statistic: {t_stat:.4f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(f\"   ✅ Significant difference detected for {metrics[i]}!\")\n",
    "    else:\n",
    "        print(f\"   ❌ No significant difference for {metrics[i]}.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Model Selection and Final Testing\n",
    "This section selects the best model based on the highest AUC score between Logistic Regression and Random Forest. The chosen model is then trained on the entire training-validation dataset and evaluated on the test set. The evaluation results are printed, providing insights into the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_model_name = \"Random Forest\" if results[\"Random Forest\"][0] > results[\"Logistic Regression\"][0] else \"Logistic Regression\"\n",
    "final_model = best_models[best_model_name]  # using the best model\n",
    "\n",
    "print(f\"\\n🚀 {best_model_name} Selected for Final Testing\")\n",
    "final_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "evaluate_on_test_set(final_model, X_test, y_test, best_model_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Overfitting Detection\n",
    "This section analyzes overfitting by plotting learning curves for the selected model. It evaluates the model's performance on both the training-validation and test datasets, helping to assess whether the model is overfitting to the training data as the training size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\n🚀 {best_model_name} Selected for Overfitting Analysis (Learning Curve)\")\n",
    "plot_learning_curves(final_model, X_train_val, y_train_val, X_test, y_test, best_model_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Manual Hyperparameter Tuning and Overfitting Detection\n",
    "Based on the results from the previous learning curve, which indicated potential overfitting, a new Random Forest model is created with manually tuned hyperparameters to address this issue. The model is then trained on the complete training-validation set. After training, learning curves are plotted to assess whether overfitting persists with the new hyperparameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RANDOM_SEED' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#he learning curve results of the optimal Random Forest model indicated some overfitting. To address this, a new Random Forest model is created using manually tuned hyperparameters to optimize the model's performance.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# ========== 允许手动输入 Random Forest 超参数 ==========\u001b[39;00m\n\u001b[0;32m      3\u001b[0m verified_rf_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m,  \u001b[38;5;66;03m# 决策树的数量\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m4\u001b[39m,  \u001b[38;5;66;03m# 限制树的最大深度，最开始为10，可以降低训练集整体的得分，目前最好值为4\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_samples_split\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m75\u001b[39m,  \u001b[38;5;66;03m# 控制最小分裂样本数,最开始为2，可以逐渐下降在数据少时训练集的得分，目前最好值为75\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbootstrap\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 是否使用自助采样\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: RANDOM_SEED,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass_weight\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# ✅ 重新创建 Random Forest 模型，使用手动超参数\u001b[39;00m\n\u001b[0;32m     13\u001b[0m final_model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mverified_rf_params)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RANDOM_SEED' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "the learning curve results of the optimal Random Forest model indicated some overfitting. \n",
    "To address this, a new Random Forest model is created using manually tuned hyperparameters to optimize the model's performance.\n",
    "'''\n",
    "\n",
    "verified_rf_params = {\n",
    "    'n_estimators': 1000,  \n",
    "    'max_depth': 4,  # Limit the maximum depth of the tree.lowering it reduces overall training set performance. Currently, the best value is 4.\n",
    "    'min_samples_split': 75,  #Control the minimum number of samples required to split a node. Gradually decreasing it improves performance when data is sparse. The best value is 75.\n",
    "    'bootstrap': True,  \n",
    "    'random_state': RANDOM_SEED,\n",
    "    'class_weight': 'balanced'\n",
    "}\n",
    "\n",
    "#   Recreate the Random Forest model using manually tuned hyperparameters\n",
    "final_model = RandomForestClassifier(**verified_rf_params)\n",
    "final_model = RandomForestClassifier(**verified_rf_params)\n",
    "\n",
    "\n",
    "#  Train the final model on the complete training set (training + validation)\n",
    "final_model.fit(X_train_val, y_train_val)\n",
    "\n",
    "\n",
    "#  Overfitting Detection (Plotting Learning Curves)\n",
    "print(f\"\\n🚀 Random Forest Selected for Overfitting Analysis (Learning Curve)\")\n",
    "plot_learning_curves(final_model, X_train_val, y_train_val, X_test, y_test, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
